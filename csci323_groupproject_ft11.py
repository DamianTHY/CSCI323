# -*- coding: utf-8 -*-
"""CSCI323 GroupProject FT11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rz3ZVb6yZnbqvJd871UUQV5Czj2BfPVI

# CSCI 323 Group Project
**Group**: FT 11<br>
**Topic:** Drive a taxi<br>
**Members**: <br>

1.   Chai Ming Liang 8864846
2.   Damian Tan Han Yi 9070473
3.   Lee Ming Xuan 9097016
4.   Pan Ting Yu 9097624
5.   Lai Pin Xuan 9096929
6.








---
"""

#from google.colab import drive
#drive.mount('/content/drive')

"""#**Learning to Drive: Reinforcement Learning for Gymnasium Taxi Environment**

# Introduction

# Background Theory
"""

#!pip -q install "gymnasium[toy_text]"
# mc_taxi.py
import time
import pickle
import random
from collections import defaultdict, Counter
from typing import Dict, Tuple, Optional, Union

import numpy as np
import gymnasium as gym

import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from matplotlib.colors import LinearSegmentedColormap

# Imports
import time
import pickle
import random
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Optional, Union

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from matplotlib.colors import LinearSegmentedColormap
import gymnasium as gym


# Reward shaping (editable)

STEP_PENALTY            = -1
PICKUP_REWARD           = +2
DROP_REWARD_PER_PAX     = +20
EPISODE_SUCCESS_BONUS   = +0
INVALID_DROPOFF_PENALTY = -10

_LOCS = [(0,0), (0,4), (4,0), (4,3)]

SAVE_PLOTS   = True         # True = save PNGs, False = display in output
PLOT_DIR     = "."          # same folder as the notebook/code
PLOT_PREFIX  = "fig"        # filename prefix
PLOT_FMT     = "png"        # "png" | "jpg" | "pdf"
PLOT_DPI     = 150

import os, itertools

if SAVE_PLOTS:
    _orig_show = plt.show
    _counter = itertools.count(1)

    def _save_all_open_figs(*args, **kwargs):
        os.makedirs(PLOT_DIR, exist_ok=True)
        # save every open figure in creation order
        for num in plt.get_fignums():
            fig = plt.figure(num)
            idx = next(_counter)
            fname = f"{PLOT_PREFIX}_{idx:02d}.{PLOT_FMT}"
            fig.savefig(os.path.join(PLOT_DIR, fname), dpi=PLOT_DPI, bbox_inches="tight")
            plt.close(fig)  # free memory

    plt.show = _save_all_open_figs
    
def shaped_delta_for_state_action(env, s: int, a: int, terminated_after: bool = False) -> float:
    """Return the EXTRA reward to add to the base Taxi reward, based on your shaping rules."""
    tr, tc, pl, di = env.unwrapped.decode(s)
    delta = (STEP_PENALTY - (-1))

    at_pickup_cell = (pl < 4) and ((tr, tc) == _LOCS[pl])
    pickup_valid   = (a == 4) and at_pickup_cell and (pl < 4)
    drop_valid     = (a == 5) and (pl == 4) and ((tr, tc) == _LOCS[di])
    illegal_any    = (a in (4,5)) and not (pickup_valid or drop_valid)

    if drop_valid:
        delta += (DROP_REWARD_PER_PAX - 20)
    if pickup_valid:
        delta += PICKUP_REWARD
    if illegal_any:
        delta += (INVALID_DROPOFF_PENALTY - (-10))
    if terminated_after and drop_valid:
        delta += EPISODE_SUCCESS_BONUS

    return delta


class RewardTuner(gym.Wrapper):
    """
    Wraps Taxi-v3 to apply reward shaping ONLINE and annotate info with event flags.
    Edit the constants or 'shaped_delta_for_state_action' to change shaping.
    """
    def __init__(self, env):
        super().__init__(env)
        self._last_state = None

    def reset(self, **kwargs):
        s, info = self.env.reset(**kwargs)
        self._last_state = s
        return s, info

    def step(self, a: int):
      s_prev = self._last_state
      s, r, terminated, truncated, info = self.env.step(a)
      info = dict(info) if info is not None else {}

      tr, tc, pl, di = self.env.unwrapped.decode(s_prev)
      at_pickup_cell = (pl < 4) and ((tr, tc) == _LOCS[pl])
      pickup_valid   = (a == 4) and at_pickup_cell and (pl < 4)
      drop_valid     = (a == 5) and (pl == 4) and ((tr, tc) == _LOCS[di])
      illegal_any    = ((a in (4,5)) and not (pickup_valid or drop_valid))

      # shaping
      r += (STEP_PENALTY - (-1))
      if drop_valid:  r += (DROP_REWARD_PER_PAX - 20)
      if pickup_valid: r += PICKUP_REWARD
      if illegal_any:  r += (INVALID_DROPOFF_PENALTY - (-10))
      if terminated and drop_valid: r += EPISODE_SUCCESS_BONUS

      # annotation
      info["pickup_valid"] = pickup_valid
      info["drop_valid"]   = drop_valid
      info["illegal_any"]  = illegal_any
      info["__terminated"] = bool(terminated)
      info["__truncated"]  = bool(truncated)

      self._last_state = s
      return s, r, terminated, truncated, info




# Base agent + MC/QL/SARSA

class BaseAgent:
    def __init__(self, nA: int, eps_start=1.0, eps_end=0.05, glie_c: Optional[float] = 5000.0, seed: Optional[int] = None):
        self.nA = nA
        self.eps_start = eps_start
        self.eps_end   = eps_end
        self.glie_c    = glie_c
        self.eps = eps_start
        self._episodes_seen = 0
        self.rng = np.random.default_rng(seed)

    def set_eps_for_episode(self):
        self._episodes_seen += 1
        if self.glie_c is not None:
            self.eps = max(self.eps_end, self.glie_c / (self.glie_c + self._episodes_seen))
        else:
            self.eps = self.eps_start

    def act_from_probs(self, probs: np.ndarray) -> int:
        return int(self.rng.choice(self.nA, p=probs))

    def act_eps_greedy_from_Q(self, Qrow: np.ndarray) -> int:
        if self.rng.random() < self.eps:
            return int(self.rng.integers(self.nA))
        m = Qrow.max()
        best = np.flatnonzero(Qrow == m)
        return int(self.rng.choice(best))

def plot_training_curve(scores, window=100, title="Training Progress"):
    import numpy as np
    import matplotlib.pyplot as plt

    scores = np.asarray(scores, dtype=float)
    plt.figure(figsize=(10,4))

    # per-episode reward
    plt.plot(np.arange(1, len(scores)+1), scores, alpha=0.35, label="Reward per Episode")

    # 100-episode moving average
    if len(scores) >= window:
        w = np.ones(window) / window
        ma = np.convolve(scores, w, mode="valid")
        x_ma = np.arange(window, len(scores)+1)
        plt.plot(x_ma, ma, label=f"{window}-Episode Moving Average")

    plt.title(title)
    plt.xlabel("Episode")
    plt.ylabel("Total Reward")
    plt.legend()
    plt.tight_layout()
    plt.show()
def plot_training_reward_and_eps(scores, eps, window=100, title="Training Progress"):
    import numpy as np
    import matplotlib.pyplot as plt

    scores = np.asarray(scores, dtype=float)
    eps    = np.asarray(eps, dtype=float)
    x = np.arange(1, len(scores)+1)

    fig, ax1 = plt.subplots(figsize=(10,4))

    # Reward curve (+ moving average)
    l_reward, = ax1.plot(x, scores, alpha=0.35, label="Reward/ep")
    if len(scores) >= window:
        w = np.ones(window)/window
        ma = np.convolve(scores, w, mode="valid")
        l_ma, = ax1.plot(np.arange(window, len(scores)+1), ma, label=f"{window}-ep MA")
    ax1.set_xlabel("Episode")
    ax1.set_ylabel("Reward")

    # Epsilon on twin axis
    ax2 = ax1.twinx()
    l_eps, = ax2.plot(x, eps, linestyle="--", label="ε (epsilon)")
    ax2.set_ylabel("Epsilon")

    # ---- Single combined legend, middle-left ----
    handles = [l_reward]
    if len(scores) >= window: handles.append(l_ma)
    handles.append(l_eps)
    labels  = [h.get_label() for h in handles]
    ax1.legend(
        handles, labels,
        loc="center right",           # middle of left side
        bbox_to_anchor=(0.95, 0.5),  # slight inset; tweak if needed
        frameon=True
    )

    plt.title(title)
    plt.tight_layout()
    plt.show()


class MonteCarloEpsilonSoft(BaseAgent):
    """
    On-policy ε-soft Monte Carlo control.
    - If alpha is a float: constant step-size MC (every-visit or first-visit per flag).
    - If alpha is None: sample-average MC (α = 1/N(s,a)), every-visit or first-visit per flag.
    """
    def __init__(self, nS: int, nA: int, gamma=0.99,
                 alpha: Optional[float] = 0.07,
                 first_visit: bool = False,
                 eps_start=1.0, eps_end=0.05, glie_c: Optional[float]=5000.0, seed: Optional[int]=None):
        super().__init__(nA, eps_start=eps_start, eps_end=eps_end, glie_c=glie_c, seed=seed)
        self.nS = nS
        self.gamma = gamma
        self.alpha = alpha
        self.first_visit = first_visit
        self.Q = defaultdict(lambda: np.zeros(nA, dtype=np.float64))
        self.N = defaultdict(lambda: np.zeros(nA, dtype=np.int64))
        self.policy: Dict[int, np.ndarray] = {s: np.ones(nA, dtype=np.float64)/nA for s in range(nS)}

    def probs_from_Q(self, s: int) -> np.ndarray:
        Qs = self.Q[s]
        a_star = np.flatnonzero(Qs == Qs.max())
        probs = np.full(self.nA, self.eps / self.nA, dtype=np.float64)
        for a in a_star:
            probs[a] += (1.0 - self.eps) / len(a_star)
        return probs

    def update_policy_row(self, s: int):
        self.policy[s] = self.probs_from_Q(s)

    def generate_episode(self, env: gym.Env, max_steps: int = 200):
      traj = []; G = 0.0; steps = 0; success = False
      s, _ = env.reset()
      done = False
      last_terminated = False
      last_truncated  = False

      while not done and steps < max_steps:
          probs = self.policy[int(s)]
          a = self.act_from_probs(probs)
          ns, r, terminated, truncated, info = env.step(a)
          done = terminated or truncated

          traj.append((int(s), int(a), float(r)))
          G += r; steps += 1; s = ns

          last_terminated = bool(terminated)
          last_truncated  = bool(truncated)

      success = last_terminated
      return traj, float(G), steps, success


    def mc_update(self, trajectory: list):
        """
        Backward return sweep. Supports:
          - first-visit (skip if (s,a) seen earlier in episode)
          - every-visit (update all occurrences)
          - sample-average (alpha=None) vs constant step-size (alpha=float)
        """
        G = 0.0
        visited = set()
        for t in range(len(trajectory) - 1, -1, -1):
            s, a, r = trajectory[t]
            G = self.gamma * G + r

            if self.first_visit:
                key = (s, a)
                if key in visited:
                    continue
                visited.add(key)

            if self.alpha is None:
                # sample-average: α = 1 / N(s,a)
                self.N[s][a] += 1
                step = 1.0 / self.N[s][a]
                self.Q[s][a] += step * (G - self.Q[s][a])
            else:
                # constant step-size
                self.Q[s][a] += self.alpha * (G - self.Q[s][a])

            self.update_policy_row(s)


class QLearningAgent(BaseAgent):
    def __init__(self, nS: int, nA: int, alpha: float, gamma: float,
                 eps_start=1.0, eps_end=0.05, glie_c: Optional[float]=5000.0, seed: Optional[int]=None):
        super().__init__(nA, eps_start=eps_start, eps_end=eps_end, glie_c=glie_c, seed=seed)
        self.nS = nS; self.alpha = alpha; self.gamma = gamma
        self.Q = defaultdict(lambda: np.zeros(nA, dtype=np.float64))

    def greedy_action(self, s: int) -> int:
        Qs = self.Q[s]; m = Qs.max(); best = np.flatnonzero(Qs == m)
        return int(self.rng.choice(best))

    def learn_episode(self, env: gym.Env, max_steps: int = 200):
      s, _ = env.reset()
      done = False; steps = 0; G = 0.0
      last_terminated = False
      last_truncated  = False

      while not done and steps < max_steps:
          a = self.act_eps_greedy_from_Q(self.Q[int(s)])
          ns, r, terminated, truncated, info = env.step(a)
          done = terminated or truncated

          target = r if done else r + self.gamma * np.max(self.Q[int(ns)])
          self.Q[int(s)][a] += self.alpha * (target - self.Q[int(s)][a])

          G += r; steps += 1; s = ns
          last_terminated = bool(terminated)
          last_truncated  = bool(truncated)

      success = last_terminated
      return float(G), steps, success


    def greedy_policy(self) -> Dict[int, int]:
        return {s: int(np.argmax(self.Q[s])) for s in range(self.nS)}


class SarsaAgent(BaseAgent):
    def __init__(self, nS: int, nA: int, alpha: float, gamma: float,
                 eps_start=1.0, eps_end=0.05, glie_c: Optional[float]=5000.0, seed: Optional[int]=None):
        super().__init__(nA, eps_start=eps_start, eps_end=eps_end, glie_c=glie_c, seed=seed)
        self.nS = nS; self.alpha = alpha; self.gamma = gamma
        self.Q = defaultdict(lambda: np.zeros(nA, dtype=np.float64))

    def learn_episode(self, env: gym.Env, max_steps: int = 200):
      s, _ = env.reset()
      a = self.act_eps_greedy_from_Q(self.Q[int(s)])
      done = False; steps = 0; G = 0.0
      last_terminated = False
      last_truncated  = False

      while not done and steps < max_steps:
          ns, r, terminated, truncated, info = env.step(a)
          done = terminated or truncated
          na = None if done else self.act_eps_greedy_from_Q(self.Q[int(ns)])
          target = r if done else r + self.gamma * self.Q[int(ns)][na]
          self.Q[int(s)][a] += self.alpha * (target - self.Q[int(s)][a])

          G += r; steps += 1; s = ns; a = na if not done else a
          last_terminated = bool(terminated)
          last_truncated  = bool(truncated)

      success = last_terminated
      return float(G), steps, success


    def greedy_policy(self) -> Dict[int, int]:
        return {s: int(np.argmax(self.Q[s])) for s in range(self.nS)}


# Value Iteration (planning)
def value_iteration(env, gamma=0.99, theta=1e-8, max_iter=10000):
    P = env.unwrapped.P
    nS = env.observation_space.n
    nA = env.action_space.n
    V = np.zeros(nS, dtype=np.float64)
    for _ in range(max_iter):
        delta = 0.0
        for s in range(nS):
            q = np.zeros(nA, dtype=np.float64)
            for a in range(nA):
                for p, ns, r_base, done in P[s][a]:
                    r = r_base + shaped_delta_for_state_action(env, s, a, terminated_after=done)
                    q[a] += p * (r + (0.0 if done else gamma * V[ns]))
            nv = np.max(q)
            delta = max(delta, abs(nv - V[s]))
            V[s] = nv
        if delta < theta:
            break
    pi = np.zeros(nS, dtype=np.int64)
    for s in range(nS):
        q = np.zeros(nA, dtype=np.float64)
        for a in range(nA):
            for p, ns, r_base, done in P[s][a]:
                r = r_base + shaped_delta_for_state_action(env, s, a, terminated_after=done)
                q[a] += p * (r + (0.0 if done else gamma * V[ns]))
        pi[s] = int(np.argmax(q))
    return V, pi


# Train functions (MC/QL/SARSA/VI)
def _log_every_10pct(ep: int, episodes: int, last_block_start: float, epsilon: float, G: float, steps: int, success: bool):
    pad = len(str(episodes))
    block_elapsed = time.time() - last_block_start
    mean_ep_time = block_elapsed / (episodes / 10.0)
    status = "SUCCESS" if success else "FAIL"
    print(f"[EP {ep:>{pad}}/{episodes}]  ε={epsilon:.3f}  Return={G:+.1f}  Steps={steps}  "
          f"Status={status:>7}  Elapsed(last block)={block_elapsed:6.2f}s  Mean/ep≈{mean_ep_time:8.4f}s")
    return time.time()

def train_mc(
    episodes: int = 20000,
    max_steps: int = 200,
    gamma: float = 0.99,
    alpha: Optional[float] = 0.07,
    first_visit: bool = False,
    eps_start: float = 1.0,
    eps_end: float = 0.05,
    glie_c: Optional[float] = 5000.0,
    seed: int = 7,
    save_path: Optional[str] = "policy_mc.pkl",
    render_mode: Optional[str] = None
):
    random.seed(seed); np.random.seed(seed)
    base_env = gym.make("Taxi-v3", render_mode=render_mode)
    env = RewardTuner(base_env)

    nS = env.observation_space.n; nA = env.action_space.n
    agent = MonteCarloEpsilonSoft(
        nS, nA, gamma=gamma,
        alpha=alpha,
        first_visit=first_visit,
        eps_start=eps_start, eps_end=eps_end, glie_c=glie_c, seed=seed
    )
    for s in range(nS): agent.update_policy_row(s)
    checkpoints = set(max(1, int(round(episodes * k / 10))) for k in range(1, 11))
    last_block_start = time.time()

    train_scores, train_steps, train_eps = [], [], []
    
    for ep in range(1, episodes + 1):
        agent.set_eps_for_episode()
        traj, G, steps, success = agent.generate_episode(env, max_steps=max_steps)
        agent.mc_update(traj)
        train_scores.append(G)
        train_steps.append(steps)
        train_eps.append(agent.eps)
        if ep in checkpoints:
            last_block_start = _log_every_10pct(ep, episodes, last_block_start, agent.eps, G, steps, success)

    if save_path:
        serializable_policy = {int(s): agent.policy[s] for s in range(nS)}
        with open(save_path, "wb") as f:
            pickle.dump(serializable_policy, f)

    return {"policy": agent.policy, "Q": agent.Q,
            "train_scores": train_scores, "train_steps": train_steps, "train_eps": train_eps}

def train_qlearning(
    episodes: int = 10000,
    max_steps: int = 200,
    alpha: float = 0.5,
    gamma: float = 0.99,
    eps_start: float = 1.0,
    eps_end: float = 0.05,
    glie_c: Optional[float] = 5000.0,
    seed: int = 7,
    save_path: Optional[str] = "policy_ql.pkl",
    render_mode: Optional[str] = None
):
    random.seed(seed); np.random.seed(seed)
    base_env = gym.make("Taxi-v3", render_mode=render_mode)
    env = RewardTuner(base_env)
    nS = env.observation_space.n; nA = env.action_space.n

    agent = QLearningAgent(nS, nA, alpha=alpha, gamma=gamma,
                           eps_start=eps_start, eps_end=eps_end, glie_c=glie_c, seed=seed)
    checkpoints = set(max(1, int(round(episodes * k / 10))) for k in range(1, 11))
    last_block_start = time.time()

    train_scores, train_steps, train_eps = [], [], []
    
    for ep in range(1, episodes + 1):
        agent.set_eps_for_episode()
        G, steps, success = agent.learn_episode(env, max_steps=max_steps)
        train_scores.append(G)
        train_steps.append(steps)
        train_eps.append(agent.eps)
        if ep in checkpoints:
            last_block_start = _log_every_10pct(ep, episodes, last_block_start, agent.eps, G, steps, success)

    policy_det = agent.greedy_policy()
    if save_path:
        with open(save_path, "wb") as f:
            pickle.dump(policy_det, f)
    return {"Q": agent.Q, "policy": policy_det,
            "train_scores": train_scores, "train_steps": train_steps, "train_eps": train_eps}

def train_sarsa(
    episodes: int = 10000,
    max_steps: int = 200,
    alpha: float = 0.5,
    gamma: float = 0.99,
    eps_start: float = 1.0,
    eps_end: float = 0.05,
    glie_c: Optional[float] = 5000.0,
    seed: int = 7,
    save_path: Optional[str] = "policy_sarsa.pkl",
    render_mode: Optional[str] = None
):
    random.seed(seed); np.random.seed(seed)
    base_env = gym.make("Taxi-v3", render_mode=render_mode)
    env = RewardTuner(base_env)
    nS = env.observation_space.n; nA = env.action_space.n

    agent = SarsaAgent(nS, nA, alpha=alpha, gamma=gamma,
                       eps_start=eps_start, eps_end=eps_end, glie_c=glie_c, seed=seed)
    checkpoints = set(max(1, int(round(episodes * k / 10))) for k in range(1, 11))
    last_block_start = time.time()

    train_scores, train_steps, train_eps = [], [], []
    
    for ep in range(1, episodes + 1):
        agent.set_eps_for_episode()
        G, steps, success = agent.learn_episode(env, max_steps=max_steps)
        train_scores.append(G)
        train_steps.append(steps)
        train_eps.append(agent.eps)
        if ep in checkpoints:
            last_block_start = _log_every_10pct(ep, episodes, last_block_start, agent.eps, G, steps, success)

    policy_det = agent.greedy_policy()
    if save_path:
        with open(save_path, "wb") as f:
            pickle.dump(policy_det, f)
    return {"Q": agent.Q, "policy": policy_det,
            "train_scores": train_scores, "train_steps": train_steps, "train_eps": train_eps}

def train_value_iteration(
    gamma: float = 0.99,
    theta: float = 1e-8,
    max_iter: int = 10000,
    seed: int = 7,
    save_path: Optional[str] = "policy_vi.pkl",
    render_mode: Optional[str] = None
):
    random.seed(seed); np.random.seed(seed)
    base_env = gym.make("Taxi-v3", render_mode=render_mode)
    env = RewardTuner(base_env)
    V, pi = value_iteration(env, gamma=gamma, theta=theta, max_iter=max_iter)
    policy_det = {int(s): int(a) for s, a in enumerate(pi)}
    if save_path:
        with open(save_path, "wb") as f:
            pickle.dump(policy_det, f)
    print(f"[VI] Computed greedy policy with gamma={gamma}, iterations<= {max_iter}, saved={bool(save_path)}")
    return {"V": V, "policy": policy_det}


# Play / Evaluate (plots data)
PolicyType = Dict[int, Union[np.ndarray, list, int, float]]

def _sample_from_policy_generic(
    policy_row: Union[np.ndarray, list, int, float],
    rng: np.random.Generator,
    nA: int
) -> int:
    """
    Supports:
      - np.ndarray/list of probabilities -> sample
      - int/float -> deterministic action
    """
    if isinstance(policy_row, (int, np.integer)):
        return int(policy_row)
    if isinstance(policy_row, (float, np.floating)):
        return int(policy_row)
    arr = np.asarray(policy_row, dtype=np.float64)
    arr = arr / arr.sum() if arr.sum() > 0 else np.ones_like(arr) / len(arr)
    return int(rng.choice(len(arr), p=arr))


def _choose_action_for_eval(
    policy_row: Union[np.ndarray, list, int, float],
    rng: np.random.Generator,
    nA: int,
    greedy_eval: bool
) -> int:
    if isinstance(policy_row, (int, np.integer, float, np.floating)):
        return int(policy_row)
    arr = np.asarray(policy_row, dtype=np.float64)
    if greedy_eval:
        return int(np.argmax(arr))
    arr = arr / arr.sum() if arr.sum() > 0 else np.ones_like(arr) / len(arr)
    return int(rng.choice(len(arr), p=arr))


def play_policy(
    policy: Optional[PolicyType] = None,
    *,
    policy_path: Optional[str] = None,
    episodes: int = 10,
    max_steps: int = 200,
    seed: int = 123,
    render_mode: Optional[str] = None,
    print_each: bool = True,
    collect_metrics: bool = True,
    greedy_eval: bool = False,
):
    """
    Evaluate a saved/learned policy over fresh episodes.
    - MC policy: dict[state] -> np.ndarray(nA) probabilities
    - QL/SARSA/VI policy: dict[state] -> int (greedy action)
    """
    if policy is None and policy_path is None:
        raise ValueError("Provide either a 'policy' dict or a 'policy_path' to load.")

    if policy is None:
        with open(policy_path, "rb") as f:
            loaded = pickle.load(f)
        policy = {int(s): v for s, v in loaded.items()}

    random.seed(seed); np.random.seed(seed)
    base_env = gym.make("Taxi-v3", render_mode=render_mode)
    env = RewardTuner(base_env)
    nA = env.action_space.n
    rng = np.random.default_rng(seed + 999)

    # Metrics to collect
    scores = []; steps_hist = []; eps_hist = []; status_hist = []
    steps_to_pick_hist = []; steps_after_pick_hist = []
    moves_up_hist, moves_down_hist, moves_left_hist, moves_right_hist = [], [], [], []
    GRID_H = GRID_W = 5
    heat_global = np.zeros((GRID_H, GRID_W), dtype=int)

    total_return = 0.0
    total_steps = 0
    successes = 0

    for ep in range(1, episodes + 1):
        s, _ = env.reset(seed=seed + ep)
        done = False; steps = 0; G = 0.0

        # Track last-step flags for correct labeling
        last_terminated = False
        last_truncated  = False

        # Per-episode extras
        heat_episode = np.zeros((GRID_H, GRID_W), dtype=int)
        tr0, tc0, _, _ = env.unwrapped.decode(s)
        heat_episode[tr0, tc0] += 1
        moves_up = moves_down = moves_left = moves_right = 0
        picked_step = None

        while not done and steps < max_steps:
            tr_prev, tc_prev, pl_prev, _ = env.unwrapped.decode(s)

            row = policy[int(s)]
            # Deterministic rows (QL/SARSA/VI) vs MC prob rows
            if isinstance(row, (int, np.integer, float, np.floating)):
                a = int(row)
            else:
                arr = np.asarray(row, dtype=np.float64)
                if greedy_eval:
                    a = int(np.argmax(arr))
                else:
                    arr = arr / arr.sum() if arr.sum() > 0 else np.ones_like(arr) / len(arr)
                    a = int(rng.choice(len(arr), p=arr))

            ns, r, terminated, truncated, info = env.step(a)
            done = terminated or truncated

            # movement counts
            if a == 1:   moves_up += 1     # North
            elif a == 0: moves_down += 1   # South
            elif a == 3: moves_left += 1   # West
            elif a == 2: moves_right += 1  # East

            G += r
            steps += 1

            # heat + pickup detection
            tr_new, tc_new, pl_new, _ = env.unwrapped.decode(ns)
            heat_episode[tr_new, tc_new] += 1
            if picked_step is None and (pl_prev < 4) and (pl_new == 4):
                picked_step = steps

            # remember final env flags
            last_terminated = bool(terminated)
            last_truncated  = bool(truncated)

            s = ns

        # Binary outcome: SUCCESS iff environment terminated (valid drop-off)
        success = last_terminated
        status = "SUCCESS" if success else "FAIL"

        total_return += G
        total_steps  += steps
        successes    += int(success)

        scores.append(float(G))
        steps_hist.append(int(steps))
        eps_hist.append(0.0)  # eval uses fixed policy
        status_hist.append(status)
        heat_global += heat_episode
        moves_up_hist.append(moves_up); moves_down_hist.append(moves_down)
        moves_left_hist.append(moves_left); moves_right_hist.append(moves_right)

        if picked_step is None:
            steps_to_pick_hist.append(None)
            steps_after_pick_hist.append(None)
        else:
            steps_to_pick_hist.append(picked_step)
            steps_after_pick_hist.append((steps - picked_step) if success else None)

        if print_each:
            print(f"[PLAY {ep:>3}/{episodes}] Return={G:+.1f}  Steps={steps}  Status={status}")

    avg_ret = total_return / episodes
    avg_steps = total_steps / episodes
    success_rate = successes / episodes

    if print_each:
        print(f"\nEval summary: episodes={episodes}, avg_return={avg_ret:+.2f}, "
              f"avg_steps={avg_steps:.1f}, success_rate={success_rate*100:.1f}%")

    result = {
        "episodes": episodes,
        "avg_return": avg_ret,
        "avg_steps": avg_steps,
        "success_rate": success_rate,
    }

    if collect_metrics:
        result.update({
            "scores": scores,
            "steps": steps_hist,
            "eps": eps_hist,
            "status": status_hist,
            "heat": heat_global,
            "steps_to_pick": steps_to_pick_hist,
            "steps_from_pick_to_drop": steps_after_pick_hist,
            "moves": {
                "up": moves_up_hist, "down": moves_down_hist,
                "left": moves_left_hist, "right": moves_right_hist
            }
        })

    return result




# Plot helper for eval logs
def plot_eval_metrics(eval_out: dict, title_prefix: str = "Policy (eval)"):
    episodes = eval_out["episodes"]
    x = np.arange(1, episodes + 1)

    def _nanify(lst):
        return np.array([np.nan if v is None else float(v) for v in lst], dtype=float)

    # 1) Score vs episode
    plt.figure()
    plt.plot(x, eval_out["scores"])
    plt.xlabel("Episode"); plt.ylabel("Return")
    plt.title(f"{title_prefix}: Score per episode")
    plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))
    plt.show()

    # 2) Steps vs episode
    plt.figure()
    plt.plot(x, eval_out["steps"])
    plt.xlabel("Episode"); plt.ylabel("Steps")
    plt.title(f"{title_prefix}: Steps per episode")
    plt.show()

    # 3) Epsilon vs episode
    if "eps" in eval_out:
        plt.figure()
        plt.plot(x, eval_out["eps"])
        plt.xlabel("Episode"); plt.ylabel("Epsilon")
        plt.title(f"{title_prefix}: Epsilon schedule (eval=0)")
        plt.show()

    # 4) SUCCESS vs FAIL
    c = Counter(eval_out["status"]); labels=["SUCCESS","FAIL"]; vals=[c.get(k,0) for k in labels]
    plt.figure()
    plt.bar(labels, vals)
    plt.title(f"{title_prefix}: Outcome counts")
    plt.show()

    # 5) Steps to pickup
    arr_pick = _nanify(eval_out["steps_to_pick"])
    plt.figure()
    plt.plot(x, arr_pick)
    plt.xlabel("Episode"); plt.ylabel("Steps")
    plt.title(f"{title_prefix}: Steps to pickup")
    plt.show()

    # 6) Steps from pickup to drop
    arr_drop = _nanify(eval_out["steps_from_pick_to_drop"])
    plt.figure()
    plt.plot(x, arr_drop)
    plt.xlabel("Episode"); plt.ylabel("Steps")
    plt.title(f"{title_prefix}: Steps from pickup to drop")
    plt.show()

    # 7) Move counts per episode
    plt.figure()
    plt.plot(x, eval_out["moves"]["up"],   label="Up (North)")
    plt.plot(x, eval_out["moves"]["down"], label="Down (South)")
    plt.plot(x, eval_out["moves"]["left"], label="Left (West)")
    plt.plot(x, eval_out["moves"]["right"],label="Right (East)")
    plt.xlabel("Episode"); plt.ylabel("Count")
    plt.title(f"{title_prefix}: Move counts per episode")
    plt.legend()
    plt.show()

    # 8) Heatmap of taxi positions
    heat_global = eval_out["heat"]
    red_cmap = LinearSegmentedColormap.from_list("white_to_red", ["#ffffff", "#ff0000"])
    plt.figure()
    vmax = int(np.max(heat_global)) or 1
    plt.imshow(
        heat_global, origin="upper",
        cmap=red_cmap, vmin=0, vmax=vmax,
        interpolation="nearest", aspect="equal"
    )
    plt.colorbar(label="Visits")
    plt.title(f"{title_prefix}: Taxi position heatmap (all episodes)")
    plt.xticks(range(5), range(5)); plt.yticks(range(5), range(5))
    for y in range(6):
        plt.axhline(y - 0.5, color="black", linewidth=0.2, alpha=0.4)
    for xx in range(6):
        plt.axvline(xx - 0.5, color="black", linewidth=0.2, alpha=0.4)
    plt.show()

# --- Monte Carlo (ε-soft) ---
#TRAIN MC
out = train_mc(
    episodes=30000,
    max_steps=200,
    gamma=0.99,
    alpha=0.0001,
    first_visit=False,
    eps_start=1.0,
    eps_end=0.00,
    glie_c=5000.0,
    seed=7,
    save_path="policy_mc.pkl",
    render_mode=None
)
plot_training_reward_and_eps(out["train_scores"], out["train_eps"], window=100,
                             title="Training Progress of Taxi Agent (MC)")
#TEST MC
mc_eval = play_policy(
    policy_path="policy_mc.pkl",
    episodes=300,
    max_steps=200,
    seed=2025,
    render_mode=None,
    print_each=True,
    collect_metrics=True,
    greedy_eval=True
)
plot_eval_metrics(mc_eval, title_prefix="MC Policy (eval)")

# TRAIN QL
out = train_qlearning(
    episodes=30000,
    max_steps=200,
    alpha=0.05,
    gamma=0.99,
    eps_start=1.0,
    eps_end=0.02,
    glie_c=5000.0,
    seed=7,
    save_path="policy_ql.pkl",
    render_mode=None
)
plot_training_reward_and_eps(out["train_scores"], out["train_eps"], window=100,
                             title="Training Progress of Taxi Agent (QL)")
#TEST QL
ql_eval = play_policy(policy_path="policy_ql.pkl",
    episodes=300,
    max_steps=200,
    seed=2025,
    render_mode=None,
    print_each=True,
    collect_metrics=True
)
plot_eval_metrics(ql_eval, title_prefix="Q-Learning Policy (eval)")

#TRAIN SARSA
out = train_sarsa(
    episodes=30000,
    max_steps=200,
    alpha=0.05,
    gamma=0.99,
    eps_start=1.0, eps_end=0.05,
    glie_c=5000.0,
    seed=7,
    save_path="policy_sarsa.pkl"
)
plot_training_reward_and_eps(out["train_scores"], out["train_eps"], window=100,
                             title="Training Progress of Taxi Agent (SARSA)")
# TEST SARSA
sarsa_eval = play_policy(policy_path="policy_sarsa.pkl",
    episodes=300,
    max_steps=200,
    seed=2025,
    render_mode=None,
    print_each=True,
    collect_metrics=True
)
plot_eval_metrics(sarsa_eval, title_prefix="SARSA Policy (eval)")

# TRAIN VI
out = train_value_iteration(
    gamma=0.99,
    theta=1e-9,
    max_iter=20000,
    seed=7,
    save_path="policy_vi.pkl"
)

#TEST VI
vi_eval = play_policy(policy_path="policy_vi.pkl",
    episodes=300,
    max_steps=200,
    seed=2025,
    render_mode=None,
    print_each=True,
    collect_metrics=True
)
plot_eval_metrics(vi_eval, title_prefix="Value Iteration Policy (eval)")

"""# Solutions, Evaluations, and Discussions"""

# Report Plots
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator

# helpers
def _arr(x):
    return np.asarray(x, dtype=float)

def moving_avg(y, k=50):
    y = _arr(y)
    if k <= 1 or k > len(y):
        return y
    w = np.ones(k)/k
    return np.convolve(y, w, mode="valid")

def last_window(y, k=500):
    y = _arr(y)
    k = min(k, len(y))
    return y[-k:]

def summary_line(name, eval_out):
    return f"{name:16s} | avg_return={eval_out['avg_return']:+7.2f} | avg_steps={eval_out['avg_steps']:.1f} | success={eval_out['success_rate']*100:5.1f}%"

def get_series(eval_out):
    scores = _arr(eval_out.get("scores", []))
    steps  = _arr(eval_out.get("steps", eval_out.get("steps_hist", [])))
    return scores, steps

def show_grid():
    plt.grid(True, alpha=0.25, linewidth=0.8)

# 1) Individual algorithm plots
def plot_individual(eval_out, title, smooth_k=50):
    scores, steps = get_series(eval_out)
    fig, axs = plt.subplots(1, 2, figsize=(12, 4))
    x = np.arange(1, len(scores)+1)
    axs[0].plot(x, scores, alpha=0.35)
    if len(scores) >= smooth_k:
        xs = np.arange(smooth_k, len(scores)+1)
        axs[0].plot(xs, moving_avg(scores, smooth_k))
    axs[0].set_title(f"{title}: Return vs Episode")
    axs[0].set_xlabel("Episode"); axs[0].set_ylabel("Return")
    axs[0].yaxis.set_major_locator(MaxNLocator(integer=True)); show_grid()

    axs[1].plot(x, steps, alpha=0.35)
    if len(steps) >= smooth_k:
        xs = np.arange(smooth_k, len(steps)+1)
        axs[1].plot(xs, moving_avg(steps, smooth_k))
    axs[1].set_title(f"{title}: Steps vs Episode")
    axs[1].set_xlabel("Episode"); axs[1].set_ylabel("Steps")
    show_grid()
    fig.suptitle(summary_line(title, eval_out))
    plt.tight_layout()
    plt.show()

print("=== Individual Algorithms ===")
plot_individual(mc_eval,    "Monte Carlo")
plot_individual(sarsa_eval, "SARSA")
plot_individual(ql_eval,    "Q-Learning")
plot_individual(vi_eval,    "Value Iteration", smooth_k=1)

# 2) On-policy vs Off-policy
def plot_on_off_policy_curves(on_list, off_list, smooth_k=100):
    on_scores = [get_series(r)[0] for r in on_list]
    off_scores = [get_series(r)[0] for r in off_list]
    L = min(*(len(s) for s in on_scores+off_scores))
    on_mean  = np.mean([s[:L] for s in on_scores], axis=0)
    off_mean = np.mean([s[:L] for s in off_scores], axis=0)

    fig, ax = plt.subplots(figsize=(10,4))
    x = np.arange(1, L+1)
    ax.plot(x, on_mean, alpha=0.35, label="On-policy (MC+SARSA)")
    ax.plot(x, off_mean, alpha=0.35, label="Off-policy (QL+VI)")
    if L >= smooth_k:
        xs = np.arange(smooth_k, L+1)
        ax.plot(xs, moving_avg(on_mean, smooth_k), label="On-policy (smoothed)")
        ax.plot(xs, moving_avg(off_mean, smooth_k), label="Off-policy (smoothed)")
    ax.set_title("On-policy vs Off-policy — Average Return vs Episode")
    ax.set_xlabel("Episode"); ax.set_ylabel("Return"); show_grid()
    ax.legend()
    plt.show()

def plot_on_off_policy_bars(on_list, off_list):
    on_returns = np.mean([np.mean(last_window(get_series(r)[0])) for r in on_list])
    on_steps   = np.mean([np.mean(last_window(get_series(r)[1])) for r in on_list])
    off_returns= np.mean([np.mean(last_window(get_series(r)[0])) for r in off_list])
    off_steps  = np.mean([np.mean(last_window(get_series(r)[1])) for r in off_list])

    fig, axs = plt.subplots(1,2, figsize=(10,4))
    axs[0].bar(["On-policy","Off-policy"], [on_returns, off_returns])
    axs[0].set_ylim(min(on_returns, off_returns) - 2, None)
    axs[0].set_title("Mean Return (last window)")
    axs[0].set_ylabel("Return"); show_grid()

    axs[1].bar(["On-policy","Off-policy"], [on_steps, off_steps])
    axs[1].set_title("Mean Steps (last window)")
    axs[1].set_ylabel("Steps"); show_grid()

    plt.suptitle("On-policy vs Off-policy — Aggregated")
    plt.tight_layout()
    plt.show()

print("=== On-policy vs Off-policy ===")
on_list  = [mc_eval, sarsa_eval]
off_list = [ql_eval, vi_eval]
plot_on_off_policy_curves(on_list, off_list)
plot_on_off_policy_bars(on_list, off_list)

# 3) Model-free vs Model-based
def plot_model_free_vs_based_bars(modelfree_list, modelbased_list):
    mf_ret = np.mean([np.mean(last_window(get_series(r)[0])) for r in modelfree_list])
    mf_stp = np.mean([np.mean(last_window(get_series(r)[1])) for r in modelfree_list])
    mb_ret = np.mean([np.mean(last_window(get_series(r)[0])) for r in modelbased_list])
    mb_stp = np.mean([np.mean(last_window(get_series(r)[1])) for r in modelbased_list])

    fig, axs = plt.subplots(1,2, figsize=(10,4))
    axs[0].bar(["Model-free","Model-based"], [mf_ret, mb_ret])
    axs[0].set_title("Mean Return (last window)")
    axs[0].set_ylabel("Return"); show_grid()

    axs[1].bar(["Model-free","Model-based"], [mf_stp, mb_stp])
    axs[1].set_title("Mean Steps (last window)")
    axs[1].set_ylabel("Steps"); show_grid()

    plt.suptitle("Model-free vs Model-based — Aggregated")
    plt.tight_layout()
    plt.show()

print("=== Model-free vs Model-based ===")
model_free  = [mc_eval, sarsa_eval, ql_eval]
model_based = [vi_eval]
plot_model_free_vs_based_bars(model_free, model_based)

# 4) Variance & Heatmaps
def plot_boxplots_returns(results_dict, labels, window=500):
    rets = [last_window(get_series(r)[0], window) for r in results_dict]
    fig, ax = plt.subplots(figsize=(10,4))
    ax.boxplot(rets, tick_labels=labels, showmeans=True)
    ax.set_title(f"Return Distribution (last {min(window, len(rets[0]))} episodes)")
    ax.set_ylabel("Return"); show_grid()
    plt.show()

def plot_heatmaps(results_dict, labels):
    fig, axs = plt.subplots(1, len(results_dict), figsize=(3.2*len(results_dict), 3.2))
    if len(results_dict) == 1: axs = [axs]
    for ax, r, lab in zip(axs, results_dict, labels):
        heat = r.get("heat", None)
        if heat is None:
            ax.set_title(f"{lab}: (no heat)")
            ax.axis("off"); continue
        im = ax.imshow(heat, origin="upper", interpolation="nearest", aspect="equal")
        ax.set_title(lab); ax.set_xticks(range(5)); ax.set_yticks(range(5))
        for y in range(6):
            ax.axhline(y-0.5, color="black", linewidth=0.2, alpha=0.4)
        for x in range(6):
            ax.axvline(x-0.5, color="black", linewidth=0.2, alpha=0.4)
    fig.colorbar(im, ax=axs, fraction=0.02)
    plt.suptitle("Taxi Position Heatmaps (all eval episodes)")
    plt.show()

print("=== Variance & Heatmaps ===")
plot_boxplots_returns([mc_eval, sarsa_eval, ql_eval, vi_eval],
                      ["MC","SARSA","QL","VI"], window=300)
plot_heatmaps([mc_eval, sarsa_eval, ql_eval, vi_eval],
              ["MC","SARSA","QL","VI"])

# 5) Compact console summary
print("\n=== Compact evaluation summary ===")
for name, res in [("Monte Carlo", mc_eval), ("SARSA", sarsa_eval),
                  ("Q-Learning", ql_eval), ("Value Iteration", vi_eval)]:
    print(summary_line(name, res))



import numpy as np, matplotlib.pyplot as plt, math, random, pickle

def tail_mean(y, k=500):
    y = np.asarray(y, float)
    k = min(k, len(y))
    return float(np.mean(y[-k:])) if len(y) else float("nan")

def get_scores_from_out(out):
    # Try common keys; fall back to evaluating the saved/returned policy
    for k in ("train_scores", "scores"):
        if k in out and out[k] is not None:
            return list(out[k])
    # fallback: evaluate policy to get returns (not training, but useful)
    if "policy" in out:
        tmp = "_tmp_policy.pkl"
        with open(tmp, "wb") as f: pickle.dump(out["policy"], f)
        ev = play_policy(policy_path=tmp, episodes=200, greedy_eval=True, print_each=False)
        # fabricate a “training-like” sequence from eval scores
        return ev.get("scores", [ev.get("avg_return", float("nan"))])
    return []

def run_once(alg, alpha, seed, **kw):
    if alg == "ql":
        return train_qlearning(alpha=float(alpha), seed=seed, save_path=None, **kw)
    if alg == "sarsa":
        return train_sarsa(alpha=float(alpha), seed=seed, save_path=None, **kw)
    if alg == "mc":
        # allow alpha=None for sample-average MC
        return train_mc(alpha=alpha, seed=seed, save_path=None, **kw)
    raise ValueError("alg must be 'ql', 'sarsa', or 'mc'")

def alpha_sweep_min(alg, alphas, seeds, **train_kw):
    means, ci95 = [], []
    for a in alphas:
        vals = []
        for s in seeds:
            random.seed(s); np.random.seed(s)
            out = run_once(alg, a, s, **train_kw)
            scores = get_scores_from_out(out)
            vals.append(tail_mean(scores, k=500))
        m = np.nanmean(vals)
        sd = np.nanstd(vals, ddof=1)
        n = np.sum(~np.isnan(vals))
        means.append(m)
        ci95.append(1.96*sd/math.sqrt(n) if n>1 else 0.0)
    return np.array(means), np.array(ci95)

def plot_alpha_curve(alphas, means, ci95, title):
    plt.errorbar(alphas, means, yerr=ci95, marker="o", capsize=3)
    plt.xlabel("alpha"); plt.ylabel("mean return (last 500 eps)")
    plt.title(title); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()

alphas = [0.02, 0.05, 0.1, 0.2, 0.3]
seeds  = [7]

means_q, ci_q = alpha_sweep_min(
    "ql", alphas, seeds,
    episodes=50000, max_steps=200,
    gamma=0.99, eps_start=1.0, eps_end=0.05, glie_c=5000.0
)
plot_alpha_curve(alphas, means_q, ci_q, "Q-Learning: alpha sensitivity")

# SARSA
means_sa, ci_sa = alpha_sweep_min(
    "sarsa", alphas, seeds,
    episodes=50000, max_steps=200,
    gamma=0.99, eps_start=1.0, eps_end=0.05, glie_c=5000.0
)
plot_alpha_curve(alphas, means_sa, ci_sa, "SARSA: alpha sensitivity")

# Monte Carlo (include sample-average)
alphas_mc = [0.001, 0.005, 0.01, 0.05, 0.1]
means_mc, ci_mc = alpha_sweep_min(
    "mc", alphas_mc, seeds[:3],
    episodes=50000, max_steps=200,
    gamma=0.99, eps_start=1.0, eps_end=0.05, glie_c=5000.0
)
plot_alpha_curve(alphas_mc, means_mc, ci_mc, "MC: alpha sensitivity")
fig, ax = plt.subplots()

ax.errorbar(alphas,     means_q,  yerr=ci_q,  marker="o", capsize=3, label="Q-Learning")
ax.errorbar(alphas,     means_sa, yerr=ci_sa, marker="s", capsize=3, label="SARSA")
ax.errorbar(alphas_mc,  means_mc, yerr=ci_mc, marker="^", capsize=3, label="Monte Carlo")

ax.set_xlabel("alpha")
ax.set_ylabel("mean return (last 500 eps)")
ax.set_title("Alpha sensitivity comparison")
ax.grid(True, alpha=0.3)

# legend just outside on the right, vertically centered
ax.legend(loc="center left", bbox_to_anchor=(0.98, 0.5), frameon=True, borderaxespad=0.)

fig.tight_layout()
plt.show()